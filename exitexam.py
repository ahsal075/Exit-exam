# -*- coding: utf-8 -*-
"""exitexam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_Romg8OFiyVjpz5ULTkjzYCKQxjkRaJ

## Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.ensemble import GradientBoostingRegressor

from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error

"""## Data loading and cleaning

"""

df=pd.read_excel('/content/partpdf_1758765274581_eurovision_1998 to 2012.xlsx')
df

df.info()

df.describe()

df.isnull().sum()

numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df.isna().sum()

"""## Analytical question

Column: `points` (Numerical) For the `points` column, I replaced missing values with the median. Thischoice  is robust against outliers and skewed
distributions, which are common in scoring data. The median better preserves the central tendency, ensuring that imputed values do not disproportionately affect statistical analyses.Column: `country` (Categorical)
For the `country` column, I filled missing values using the mode (the most frequent value). This strategy keeps the data within valid categories, avoids introducing artificial categories, and minimizes bias compared to arbitrary placeholder values such as 'Unknown'.
Both approaches minimize distortion of the original data and maintain its analytical integrity for further exploration.

## Intelligent Feature Selection
"""

# List of desired audio features
desired_audio_features = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo']

# Only keep features that exist in the dataframe
available_audio_features = [feat for feat in desired_audio_features if feat in df.columns]
print("Available audio features:", available_audio_features)

audio_df = df[available_audio_features]

# Plot the correlation matrix heatmap
plt.figure(figsize=(10,8))
corr_matrix = audio_df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix: Audio Features')
plt.show()

"""## Analytical question
Kept Features are:
. Danceability
. Acousticness
. Valence
. Energy
Discarded Features:
.Loudness (high correlation with Energy, r > 0.8)
. Tempo (moderate correlation with Danceability)
. Speechiness (low variance or weak relevance)
Justification:  
From the correlation matrix heatmap, I observed that 'loudness' and 'energy' are strongly correlated (correlation coefficient above 0.8). To minimize multicollinearity, I kept 'energy' and discarded 'loudness'.  
'Danceability' and 'tempo' are moderately correlated, so I kept 'danceability' for its broader relevance in music analysis.  
'Speechiness' showed low variance and weak correlations with other features, making it less informative for further analysis.
The final feature set—'danceability', 'acousticness', 'valence', and 'energy'—provides a more independent and informative basis for modeling and analysis.

## Exploratory data analysis
"""

# Check that the required columns exist
assert 'danceability' in df.columns, "Column 'danceability' not found in dataset"
assert 'Points' in df.columns, "Column 'Points' not found in dataset"

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='danceability', y='Points', hue='Country', palette='viridis', alpha=0.7)

# Add titles and labels
plt.title('Relationship Between Danceability and Eurovision Points')
plt.xlabel('Danceability')
plt.ylabel('Points')
plt.legend(title='Country', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""## Analytical question
Based on the scatter plot of danceability vs. Points, there appears to be a positive correlation—songs with higher danceability tend to receive more points in the Eurovision contest.
Hypothesis:
"In the Eurovision Song Contest, songs with higher danceability are more likely to receive higher scores, suggesting that energetic and rhythmically engaging performances resonate better with both juries and audiences."
This could reflect the contest's emphasis on entertainment value, stage presence, and audience appeal. However, further analysis would be needed to confirm causality and account for other influential factors like genre, country, and performance order

## Model Training and Evaluation
"""

# 2. Select relevant features and target
selected_features = ['danceability', 'energy', 'acousticness', 'valence'] # Using previously selected audio features
target = 'Points'  # Using 'Points' as the target variable

X = df[selected_features]
y = df[target]

# 3. Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Train the Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)
gbr.fit(X_train, y_train)

# 5. Make predictions and evaluate MAE
y_pred = gbr.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")

"""## Analytical question

Interpreting MAE in the Context of Eurovision
The Mean Absolute Error(MAE) measures the average magnitude of errors between the predicted and actual scores, without considering their direction. In the context of the Eurovision Song Contest, where countries award points to performances (typically ranging from 0 to 12 per country, with total scores often reaching into the hundreds), MAE helps us understand how close our model's predictions are to reality.
What does this mean practically?
.If a contestant actually received 200 points, our model might have predicted 180 or 220.
.A lower MAE (e.g., 5–10) would indicate high accuracy, while a higher MAE (e.g., 30+) suggests the model struggles to capture voting patterns or performance appeal.

"""

# Assume X_train and y_train are already defined
model = GradientBoostingRegressor()
model.fit(X_train, y_train)

# Save the model to a file
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)